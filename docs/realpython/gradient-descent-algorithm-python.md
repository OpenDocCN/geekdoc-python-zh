# åŸºäº Python å’Œ NumPy çš„éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•

> åŸæ–‡ï¼š<https://realpython.com/gradient-descent-algorithm-python/>

[**éšæœºæ¢¯åº¦ä¸‹é™**](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œå¸¸ç”¨äºæœºå™¨å­¦ä¹ åº”ç”¨ä¸­ï¼Œä»¥æ‰¾åˆ°å¯¹åº”äºé¢„æµ‹å’Œå®é™…è¾“å‡ºä¹‹é—´æœ€ä½³æ‹Ÿåˆçš„æ¨¡å‹å‚æ•°ã€‚è¿™æ˜¯ä¸€ç§ä¸ç²¾ç¡®ä½†å¼ºå¤§çš„æŠ€æœ¯ã€‚

éšæœºæ¢¯åº¦ä¸‹é™å¹¿æ³›ç”¨äºæœºå™¨å­¦ä¹ åº”ç”¨ä¸­ã€‚ç»“åˆ[åå‘ä¼ æ’­](https://brilliant.org/wiki/backpropagation/)ï¼Œåœ¨[ç¥ç»ç½‘ç»œ](https://realpython.com/python-keras-text-classification/#a-primer-on-deep-neural-networks)è®­ç»ƒåº”ç”¨ä¸­å ä¸»å¯¼åœ°ä½ã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ :

*   **æ¢¯åº¦ä¸‹é™**å’Œ**éšæœºæ¢¯åº¦ä¸‹é™**ç®—æ³•å¦‚ä½•å·¥ä½œ
*   å¦‚ä½•å°†æ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™åº”ç”¨äº**æœ€å°åŒ–æœºå™¨å­¦ä¹ ä¸­çš„æŸå¤±å‡½æ•°**
*   ä»€ä¹ˆæ˜¯**å­¦ä¹ ç‡**ï¼Œä¸ºä»€ä¹ˆå®ƒå¾ˆé‡è¦ï¼Œä»¥åŠå®ƒå¦‚ä½•å½±å“ç»“æœ
*   å¦‚ä½•**ä¸ºéšæœºæ¢¯åº¦ä¸‹é™å†™è‡ªå·±çš„å‡½æ•°**

**å…è´¹å¥–åŠ±:** [æŒæ¡ Python çš„ 5 ä¸ªæƒ³æ³•](https://realpython.com/bonus/python-mastery-course/)ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘ Python å¼€å‘è€…çš„å…è´¹è¯¾ç¨‹ï¼Œå‘æ‚¨å±•ç¤ºå°† Python æŠ€èƒ½æå‡åˆ°ä¸‹ä¸€ä¸ªæ°´å¹³æ‰€éœ€çš„è·¯çº¿å›¾å’Œå¿ƒæ€ã€‚

## åŸºæœ¬æ¢¯åº¦ä¸‹é™ç®—æ³•

[æ¢¯åº¦ä¸‹é™ç®—æ³•](https://en.wikipedia.org/wiki/Gradient_descent)æ˜¯[æ•°å­¦ä¼˜åŒ–](https://en.wikipedia.org/wiki/Mathematical_optimization)çš„ä¸€ç§è¿‘ä¼¼è¿­ä»£æ–¹æ³•ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥é€¼è¿‘ä»»ä½•ä¸€ä¸ª[å¯å¾®å‡½æ•°](https://en.wikipedia.org/wiki/Differentiable_function)çš„æœ€å°å€¼ã€‚

**æ³¨:**æ•°å­¦è§„åˆ’æœ‰å¾ˆå¤šä¼˜åŒ–æ–¹æ³•å’Œ[å­é¢†åŸŸã€‚å¦‚æœä½ æƒ³å­¦ä¹ å¦‚ä½•åœ¨ Python ä¸­ä½¿ç”¨å®ƒä»¬ä¸­çš„ä¸€äº›ï¼Œé‚£ä¹ˆçœ‹çœ‹](https://en.wikipedia.org/wiki/Mathematical_optimization#Major_subfields)[ç§‘å­¦ Python:ä½¿ç”¨ SciPy è¿›è¡Œä¼˜åŒ–](https://realpython.com/python-scipy-cluster-optimize/)å’Œ[åŠ¨æ‰‹çº¿æ€§ç¼–ç¨‹:ç”¨ Python è¿›è¡Œä¼˜åŒ–](https://realpython.com/linear-programming-python/)ã€‚

è™½ç„¶æ¢¯åº¦ä¸‹é™æœ‰æ—¶ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼æˆ– T2 éç‚¹ï¼Œè€Œä¸æ˜¯æ‰¾åˆ°å…¨å±€æœ€å°å€¼ï¼Œä½†å®ƒåœ¨å®è·µä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚[æ•°æ®ç§‘å­¦](https://realpython.com/learning-paths/data-science-python-core-skills/)å’Œ[æœºå™¨å­¦ä¹ ](https://realpython.com/learning-paths/machine-learning-python/)æ–¹æ³•ç»å¸¸åœ¨å†…éƒ¨åº”ç”¨å®ƒæ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚ä¾‹å¦‚ï¼Œç¥ç»ç½‘ç»œé€šè¿‡æ¢¯åº¦ä¸‹é™æ‰¾åˆ°[æƒé‡å’Œåå·®](https://docs.paperspace.com/machine-learning/wiki/weights-and-biases)ã€‚

[*Remove ads*](/account/join/)

### æˆæœ¬å‡½æ•°:ä¼˜åŒ–çš„ç›®æ ‡

**æˆæœ¬å‡½æ•°**æˆ–[æŸå¤±å‡½æ•°](https://en.wikipedia.org/wiki/Loss_function)ï¼Œæ˜¯é€šè¿‡æ”¹å˜å†³ç­–å˜é‡æ¥æœ€å°åŒ–(æˆ–æœ€å¤§åŒ–)çš„å‡½æ•°ã€‚è®¸å¤šæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨è¡¨é¢ä¸‹è§£å†³ä¼˜åŒ–é—®é¢˜ã€‚ä»–ä»¬å€¾å‘äºé€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°(å¦‚[ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Artificial_neural_network)çš„æƒé‡å’Œåå·®ã€[éšæœºæ£®æ—](https://en.wikipedia.org/wiki/Random_forest)æˆ–[æ¢¯åº¦æ¨è¿›](https://en.wikipedia.org/wiki/Gradient_boosting)çš„å†³ç­–è§„åˆ™ç­‰)æ¥æœ€å°åŒ–å®é™…å’Œé¢„æµ‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚

åœ¨ä¸€ä¸ª[å›å½’é—®é¢˜](https://realpython.com/linear-regression-in-python/#regression)ä¸­ï¼Œä½ é€šå¸¸æœ‰è¾“å…¥å˜é‡ğ± = (ğ‘¥â‚ï¼Œâ€¦ï¼Œğ‘¥áµ£)å’Œå®é™…è¾“å‡ºğ‘¦.çš„å‘é‡æ‚¨å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œå°†ğ±æ˜ å°„åˆ°é¢„æµ‹å“åº”ğ‘“(ğ±ï¼Œä»¥ä¾¿ğ‘“(ğ±å°½å¯èƒ½æ¥è¿‘ğ‘¦.ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨ç»™å®šè¾“å…¥(å¦‚æŸäººåœ¨å…¬å¸çš„å¹´æ•°æˆ–å—æ•™è‚²ç¨‹åº¦)çš„æƒ…å†µä¸‹é¢„æµ‹ä¸€ä¸ªè¾“å‡º(å¦‚æŸäººçš„å·¥èµ„)ã€‚

æ‚¨çš„ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹ğ‘“(ğ±å’Œå®é™…æ•°æ®ğ‘¦.ä¹‹é—´çš„å·®å¼‚è¿™ä¸ªå·®å€¼ç§°ä¸º**æ®‹å·®**ã€‚

åœ¨è¿™ç§ç±»å‹çš„é—®é¢˜ä¸­ï¼Œæ‚¨å¸Œæœ›æœ€å°åŒ–æ‰€æœ‰è§‚æµ‹å€¼çš„[æ®‹å·®å¹³æ–¹å’Œ(SSR)](https://en.wikipedia.org/wiki/Residual_sum_of_squares) ï¼Œå…¶ä¸­ SSR =Ïƒáµ¢(ğ‘¦áµ¢ğ‘“(ğ±áµ¢)ã€‘ğ‘–= 1ï¼Œâ€¦ï¼Œğ‘›ï¼Œå…¶ä¸­ğ‘›æ˜¯è§‚æµ‹å€¼çš„æ€»æ•°ã€‚æˆ–è€…ï¼Œä½ å¯ä»¥ä½¿ç”¨[å‡æ–¹è¯¯å·®](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE = SSR / ğ‘›)æ¥ä»£æ›¿ SSRã€‚

SSR å’Œ MSE éƒ½ä½¿ç”¨å®é™…è¾“å‡ºå’Œé¢„æµ‹è¾“å‡ºä¹‹å·®çš„å¹³æ–¹ã€‚å·®å¼‚è¶Šå°ï¼Œé¢„æµ‹å°±è¶Šå‡†ç¡®ã€‚å·®å€¼ä¸ºé›¶è¡¨ç¤ºé¢„æµ‹å€¼ç­‰äºå®é™…æ•°æ®ã€‚

é€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°æ¥æœ€å°åŒ– SSR æˆ– MSEã€‚æ¯”å¦‚åœ¨[çº¿æ€§å›å½’](https://realpython.com/linear-regression-in-python/)ä¸­ï¼Œä½ æƒ³æ±‚å‡½æ•°ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£ï¼Œé‚£ä¹ˆä½ éœ€è¦ç¡®å®šä½¿ SSR æˆ– MSE æœ€å°åŒ–çš„æƒé‡ğ‘â‚€ï¼Œğ‘â‚ï¼Œâ€¦ï¼Œğ‘áµ£ã€‚

åœ¨[åˆ†ç±»é—®é¢˜](https://realpython.com/logistic-regression-python/#classification)ä¸­ï¼Œè¾“å‡ºğ‘¦æ˜¯[åˆ†ç±»çš„](https://en.wikipedia.org/wiki/Categorical_variable)ï¼Œé€šå¸¸ä¸º 0 æˆ– 1ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½è¯•å›¾é¢„æµ‹ä¸€å°ç”µå­é‚®ä»¶æ˜¯å¦æ˜¯åƒåœ¾é‚®ä»¶ã€‚åœ¨äºŒè¿›åˆ¶è¾“å‡ºçš„æƒ…å†µä¸‹ï¼Œæœ€å°åŒ–[äº¤å‰ç†µå‡½æ•°](https://en.wikipedia.org/wiki/Cross_entropy)æ˜¯æ–¹ä¾¿çš„ï¼Œå®ƒä¹Ÿå–å†³äºå®é™…è¾“å‡ºğ‘¦áµ¢å’Œç›¸åº”çš„é¢„æµ‹ğ‘(ğ±áµ¢):

[![mmst-gda-eqs-1](img/c9a6bda9da338af84f67959addf8231b.png)](https://files.realpython.com/media/mmst-gda-eqs-1.119ab87cc186.png)

åœ¨ç»å¸¸ç”¨äºè§£å†³åˆ†ç±»é—®é¢˜çš„[é€»è¾‘å›å½’](https://realpython.com/logistic-regression-python/)ä¸­ï¼Œå‡½æ•°ğ‘(ğ±å’Œğ‘“(ğ±å®šä¹‰å¦‚ä¸‹:

[![mmst-gda-eqs-2](img/6926ca27c08505f5ae0a68e3a74bdf78.png)](https://files.realpython.com/media/mmst-gda-eqs-2.76aa15da2cc0.png)

åŒæ ·ï¼Œä½ éœ€è¦æ‰¾åˆ°æƒé‡ğ‘â‚€ï¼Œğ‘â‚ï¼Œâ€¦ï¼Œğ‘áµ£ï¼Œä½†è¿™ä¸€æ¬¡ä»–ä»¬åº”è¯¥æœ€å°åŒ–äº¤å‰ç†µå‡½æ•°ã€‚

### å‡½æ•°çš„æ¢¯åº¦:å¾®ç§¯åˆ†å¤ä¹ å™¨

åœ¨å¾®ç§¯åˆ†ä¸­ï¼Œä¸€ä¸ªå‡½æ•°çš„[å¯¼æ•°](https://www.mathsisfun.com/calculus/derivatives-introduction.html)æ˜¾ç¤ºäº†å½“ä½ ä¿®æ”¹å®ƒçš„å‚æ•°(æˆ–å¤šä¸ªå‚æ•°)æ—¶ï¼Œä¸€ä¸ªå€¼æ”¹å˜äº†å¤šå°‘ã€‚å¯¼æ•°å¯¹äºä¼˜åŒ–å¾ˆé‡è¦ï¼Œå› ä¸º[é›¶å¯¼æ•°](http://sofia.nmsu.edu/~breakingaway/ebookofcalculus/MeaningOfDerivativesAndIntegrals/WhatDoesItMeanThatTheDerivativeOfAFunctionEquals0/WhatDoesItMeanThatTheDerivativeOfAFunctionEquals0.html)å¯èƒ½è¡¨ç¤ºæœ€å°å€¼ã€æœ€å¤§å€¼æˆ–éç‚¹ã€‚

å¤šä¸ªè‡ªå˜é‡çš„å‡½æ•°ğ¶çš„[æ¢¯åº¦](https://en.wikipedia.org/wiki/Gradient)ğ‘£â‚ï¼Œâ€¦ï¼Œğ‘£áµ£ç”¨âˆ‡ğ¶(ğ‘£â‚ï¼Œâ€¦ï¼Œğ‘£áµ£è¡¨ç¤º)å®šä¹‰ä¸ºğ¶çš„[åå¯¼æ•°](https://en.wikipedia.org/wiki/Partial_derivative)ç›¸å¯¹äºæ¯ä¸ªè‡ªå˜é‡çš„å‘é‡å‡½æ•°:âˆ‡ğ¶ = (âˆ‚ğ¶/âˆ‚ğ‘£â‚ï¼Œâ€¦ï¼Œâ‰ˆ3/3)ã€‚âˆ‡è¿™ä¸ªç¬¦å·å«åš[çº³å¸ƒæ‹‰](https://en.wikipedia.org/wiki/Nabla_symbol)ã€‚

å‡½æ•°ğ¶åœ¨ç»™å®šç‚¹çš„æ¢¯åº¦çš„éé›¶å€¼å®šä¹‰äº†ğ¶.æœ€å¿«å¢é•¿çš„æ–¹å‘å’Œé€Ÿç‡ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ—¶ï¼Œæ‚¨ä¼šå¯¹æˆæœ¬å‡½æ•°ä¸­æœ€å¿«çš„*ä¸‹é™*çš„æ–¹å‘æ„Ÿå…´è¶£ã€‚è¿™ä¸ªæ–¹å‘ç”±è´Ÿæ¢¯åº¦âˆ’âˆ‡ğ¶.å†³å®š

### æ¢¯åº¦ä¸‹é™èƒŒåçš„ç›´è§‰

ä¸ºäº†ç†è§£æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæƒ³è±¡ä¸€æ»´æ°´ä»ç¢—çš„ä¾§é¢æ»‘ä¸‹æˆ–è€…ä¸€ä¸ªçƒä»å±±ä¸Šæ»šä¸‹ã€‚æ°´æ»´å’Œçƒè¶‹å‘äºå‘ä¸‹é™æœ€å¿«çš„æ–¹å‘è¿åŠ¨ï¼Œç›´åˆ°å®ƒä»¬åˆ°è¾¾åº•éƒ¨ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä»–ä»¬ä¼šè·å¾—åŠ¨åŠ›å¹¶åŠ é€Ÿå‰è¿›ã€‚

æ¢¯åº¦ä¸‹é™èƒŒåçš„æ€æƒ³æ˜¯ç±»ä¼¼çš„:ä½ ä»ä¸€ä¸ªä»»æ„é€‰æ‹©çš„ç‚¹æˆ–å‘é‡ğ¯ = (ğ‘£â‚ï¼Œâ€¦ï¼Œğ‘£áµ£)çš„ä½ç½®å¼€å§‹ï¼Œå¹¶åœ¨æˆæœ¬å‡½æ•°ä¸‹é™æœ€å¿«çš„æ–¹å‘ä¸Šè¿­ä»£ç§»åŠ¨å®ƒã€‚å¦‚å‰æ‰€è¿°ï¼Œè¿™æ˜¯è´Ÿæ¢¯åº¦çŸ¢é‡âˆ’âˆ‡ğ¶.çš„æ–¹å‘

ä¸€æ—¦ä½ æœ‰äº†ä¸€ä¸ªéšæœºçš„èµ·ç‚¹ğ¯ = (ğ‘£â‚ï¼Œâ€¦ï¼Œğ‘£áµ£)ï¼Œä½ **æ›´æ–°**å®ƒï¼Œæˆ–è€…æŠŠå®ƒç§»åˆ°è´Ÿæ¢¯åº¦æ–¹å‘çš„ä¸€ä¸ªæ–°ä½ç½®:ğ¯â†’ğ¯ğœ‚âˆ‡ğ¶ï¼Œå…¶ä¸­ğœ‚(è¯»ä½œâ€œee-tahâ€)æ˜¯ä¸€ä¸ªå°çš„æ­£å€¼ï¼Œå«åš**å­¦ä¹ ç‡**ã€‚

å­¦ä¹ ç‡å†³å®šäº†æ›´æ–°æˆ–ç§»åŠ¨æ­¥é•¿çš„å¤§å°ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°ã€‚å¦‚æœğœ‚å¤ªå°ï¼Œé‚£ä¹ˆç®—æ³•å¯èƒ½æ”¶æ•›å¾—éå¸¸æ…¢ã€‚å¤§çš„ğœ‚å€¼è¿˜ä¼šå¯¼è‡´æ”¶æ•›é—®é¢˜æˆ–ä½¿ç®—æ³•å‘æ•£ã€‚

[*Remove ads*](/account/join/)

### åŸºæœ¬æ¢¯åº¦ä¸‹é™çš„å®ç°

ç°åœ¨æ‚¨å·²ç»çŸ¥é“äº†åŸºæœ¬çš„æ¢¯åº¦ä¸‹é™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæ‚¨å¯ä»¥ç”¨ Python å®ç°å®ƒäº†ã€‚æ‚¨å°†åªä½¿ç”¨æ™®é€š Python å’Œ [NumPy](https://numpy.org/) ï¼Œè¿™ä½¿æ‚¨èƒ½å¤Ÿåœ¨å¤„ç†æ•°ç»„(æˆ–å‘é‡)æ—¶ç¼–å†™[ç®€æ˜ä»£ç ](https://realpython.com/numpy-array-programming/)ï¼Œå¹¶è·å¾—[æ€§èƒ½æå‡](https://realpython.com/numpy-tensorflow-performance/)ã€‚

è¿™æ˜¯è¯¥ç®—æ³•çš„ä¸€ä¸ªåŸºæœ¬å®ç°ï¼Œä»ä»»æ„ç‚¹`start`å¼€å§‹ï¼Œè¿­ä»£åœ°å°†å®ƒç§»å‘æœ€å°å€¼ï¼Œ[è¿”å›](https://realpython.com/python-return-statement/)ä¸€ä¸ªæœ‰å¸Œæœ›è¾¾åˆ°æˆ–æ¥è¿‘æœ€å°å€¼çš„ç‚¹:

```py
 1def gradient_descent(gradient, start, learn_rate, n_iter):
 2    vector = start
 3    for _ in range(n_iter):
 4        diff = -learn_rate * gradient(vector)
 5        vector += diff
 6    return vector
```

`gradient_descent()`éœ€è¦å››ä¸ªå‚æ•°:

1.  **`gradient`** æ˜¯[å‡½æ•°](https://realpython.com/defining-your-own-python-function/)æˆ–ä»»ä½• Python [å¯è°ƒç”¨å¯¹è±¡](https://docs.python.org/3/reference/datamodel.html#emulating-callable-objects)ï¼Œå®ƒæ¥å—ä¸€ä¸ªå‘é‡å¹¶è¿”å›ä½ è¯•å›¾æœ€å°åŒ–çš„å‡½æ•°çš„æ¢¯åº¦ã€‚
2.  **`start`** æ˜¯ç®—æ³•å¼€å§‹æœç´¢çš„ç‚¹ï¼Œä»¥åºåˆ—([å…ƒç»„ã€åˆ—è¡¨](https://realpython.com/python-lists-tuples/)ã€ [NumPy æ•°ç»„](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)ç­‰)æˆ–æ ‡é‡(åœ¨ä¸€ç»´é—®é¢˜çš„æƒ…å†µä¸‹)çš„å½¢å¼ç»™å‡ºã€‚
3.  **`learn_rate`** æ˜¯æ§åˆ¶å‘é‡æ›´æ–°å¹…åº¦çš„å­¦ä¹ é€Ÿç‡ã€‚
4.  **`n_iter`** æ˜¯è¿­ä»£çš„æ¬¡æ•°ã€‚

è¿™ä¸ªå‡½æ•°çš„åŠŸèƒ½ä¸ä¸Šé¢ä¸­æè¿°çš„[å®Œå…¨ç›¸åŒ:å®ƒå–ä¸€ä¸ªèµ·ç‚¹(ç¬¬ 2 è¡Œ)ï¼Œæ ¹æ®å­¦ä¹ ç‡å’Œæ¢¯åº¦å€¼è¿­ä»£æ›´æ–°å®ƒ(ç¬¬ 3 åˆ° 5 è¡Œ)ï¼Œæœ€åè¿”å›æ‰¾åˆ°çš„æœ€åä¸€ä¸ªä½ç½®ã€‚](#intuition-behind-gradient-descent)

åœ¨åº”ç”¨`gradient_descent()`ä¹‹å‰ï¼Œæ‚¨å¯ä»¥æ·»åŠ å¦ä¸€ä¸ªç»ˆæ­¢æ ‡å‡†:

```py
 1import numpy as np 2
 3def gradient_descent(
 4    gradient, start, learn_rate, n_iter=50, tolerance=1e-06 5):
 6    vector = start
 7    for _ in range(n_iter):
 8        diff = -learn_rate * gradient(vector)
 9        if np.all(np.abs(diff) <= tolerance): 10            break 11        vector += diff
12    return vector
```

ç°åœ¨æ‚¨æœ‰äº†é¢å¤–çš„å‚æ•°`tolerance`(ç¬¬ 4 è¡Œ)ï¼Œå®ƒæŒ‡å®šäº†æ¯æ¬¡è¿­ä»£ä¸­å…è®¸çš„æœ€å°ç§»åŠ¨ã€‚æ‚¨è¿˜å®šä¹‰äº†`tolerance`å’Œ`n_iter`çš„é»˜è®¤å€¼ï¼Œå› æ­¤æ‚¨ä¸å¿…åœ¨æ¯æ¬¡è°ƒç”¨`gradient_descent()`æ—¶éƒ½æŒ‡å®šå®ƒä»¬ã€‚

å¦‚æœå½“å‰è¿­ä»£ä¸­çš„å‘é‡æ›´æ–°å°äºæˆ–ç­‰äº`tolerance`ï¼Œç¬¬ 9 è¡Œå’Œç¬¬ 10 è¡Œä½¿`gradient_descent()`èƒ½å¤Ÿåœæ­¢è¿­ä»£å¹¶åœ¨åˆ°è¾¾`n_iter`ä¹‹å‰è¿”å›ç»“æœã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨æœ€å°å€¼é™„è¿‘ï¼Œè¿™é‡Œçš„æ¢¯åº¦é€šå¸¸å¾ˆå°ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå®ƒä¹Ÿå¯èƒ½å‘ç”Ÿåœ¨å±€éƒ¨æœ€å°å€¼æˆ–éç‚¹é™„è¿‘ã€‚

ç¬¬ 9 è¡Œä½¿ç”¨æ–¹ä¾¿çš„ NumPy å‡½æ•° [`numpy.all()`](https://numpy.org/doc/stable/reference/generated/numpy.all.html) å’Œ [`numpy.abs()`](https://numpy.org/doc/stable/reference/generated/numpy.absolute.html) åœ¨ä¸€æ¡è¯­å¥ä¸­æ¯”è¾ƒ`diff`å’Œ`tolerance`çš„[ç»å¯¹å€¼](https://realpython.com/python-absolute-value)ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ åœ¨ 1 å·çº¿ä¸Šçš„`import numpy`ã€‚

ç°åœ¨ä½ å·²ç»æœ‰äº†ç¬¬ä¸€ä¸ªç‰ˆæœ¬çš„`gradient_descent()`ï¼Œæ˜¯æ—¶å€™æµ‹è¯•ä½ çš„åŠŸèƒ½äº†ã€‚ä½ å°†ä»ä¸€ä¸ªå°ä¾‹å­å¼€å§‹ï¼Œæ‰¾åˆ°å‡½æ•° [ğ¶ = ğ‘£](https://www.wolframalpha.com/input/?i=v**2) çš„æœ€å°å€¼ã€‚

è¿™ä¸ªå‡½æ•°åªæœ‰ä¸€ä¸ªè‡ªå˜é‡(ğ‘£)ï¼Œå®ƒçš„æ¢¯åº¦æ˜¯å¯¼æ•° 2ğ‘£.è¿™æ˜¯ä¸€ä¸ªå¯å¾®çš„[å‡¸å‡½æ•°](https://en.wikipedia.org/wiki/Convex_function)ï¼Œå¯»æ‰¾å…¶æœ€å°å€¼çš„åˆ†ææ–¹æ³•å¾ˆç®€å•ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œè§£æå¾®åˆ†å¯èƒ½æ˜¯å›°éš¾çš„ï¼Œç”šè‡³æ˜¯ä¸å¯èƒ½çš„ï¼Œå¹¶ä¸”é€šå¸¸ç”¨[æ•°å€¼æ–¹æ³•](https://en.wikipedia.org/wiki/Numerical_method)æ¥è¿‘ä¼¼ã€‚

æ‚¨åªéœ€è¦ä¸€æ¡è¯­å¥æ¥æµ‹è¯•æ‚¨çš„æ¢¯åº¦ä¸‹é™å®ç°:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.2
... )
2.210739197207331e-06
```

ä½ ä½¿ç”¨[Î»å‡½æ•°](https://realpython.com/python-lambda/) `lambda v: 2 * v`æ¥æä¾›ğ‘£çš„æ¢¯åº¦ã€‚æ‚¨ä»å€¼`10.0`å¼€å§‹ï¼Œå¹¶å°†å­¦ä¹ ç‡è®¾ç½®ä¸º`0.2`ã€‚ä½ ä¼šå¾—åˆ°ä¸€ä¸ªéå¸¸æ¥è¿‘äºé›¶çš„ç»“æœï¼Œè¿™æ˜¯æ­£ç¡®çš„æœ€å°å€¼ã€‚

ä¸‹å›¾æ˜¾ç¤ºäº†è§£å†³æ–¹æ¡ˆåœ¨è¿­ä»£è¿‡ç¨‹ä¸­çš„ç§»åŠ¨:

[![gda-perfect-updates](img/539a9707f48aa6753bc585db73458501.png)](https://files.realpython.com/media/gd-1.25c5ef2aed4e.png)

ä½ ä»æœ€å³è¾¹çš„ç»¿ç‚¹(ğ‘£ = 10)å¼€å§‹ï¼Œå‘æœ€å°å€¼(ğ‘£ = 0)ç§»åŠ¨ã€‚å› ä¸ºæ¢¯åº¦(å’Œæ–œç‡)çš„å€¼è¾ƒé«˜ï¼Œæ‰€ä»¥æ›´æ–°ä¸€å¼€å§‹è¾ƒå¤§ã€‚å½“ä½ æ¥è¿‘æœ€å°å€¼æ—¶ï¼Œå®ƒä»¬å˜å¾—æ›´ä½ã€‚

[*Remove ads*](/account/join/)

### å­¦ä¹ ç‡å½±å“

å­¦ä¹ ç‡æ˜¯ç®—æ³•çš„ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°ã€‚ä¸åŒçš„å­¦ä¹ ç‡å€¼ä¼šæ˜¾è‘—å½±å“æ¢¯åº¦ä¸‹é™çš„è¡Œä¸ºã€‚è€ƒè™‘å‰é¢çš„ä¾‹å­ï¼Œä½†æ˜¯å­¦ä¹ ç‡æ˜¯ 0.8 è€Œä¸æ˜¯ 0.2:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.8
... )
-4.77519666596786e-07
```

ä½ ä¼šå¾—åˆ°å¦ä¸€ä¸ªéå¸¸æ¥è¿‘äºé›¶çš„è§£ï¼Œä½†æ˜¯ç®—æ³•çš„å†…éƒ¨è¡Œä¸ºæ˜¯ä¸åŒçš„ã€‚è¿™å°±æ˜¯ğ‘£çš„ä»·å€¼åœ¨è¿­ä»£ä¸­å‘ç”Ÿçš„æƒ…å†µ:

[![gda-large-learning-rate](img/00bc460ba31a04ff7fee7dceeff41a11.png)](https://files.realpython.com/media/gd-3.ff9f92989807.png)

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å†æ¬¡ä»ğ‘£ = 10 å¼€å§‹ï¼Œä½†æ˜¯ç”±äºå­¦ä¹ ç‡è¾ƒé«˜ï¼Œğ‘£å‘ç”Ÿäº†å¾ˆå¤§çš„å˜åŒ–ï¼Œè½¬åˆ°äº†æœ€ä½³å€¼çš„å¦ä¸€ä¾§ï¼Œå˜æˆäº† 6ã€‚åœ¨ç¨³å®šåœ¨é›¶ç‚¹é™„è¿‘ä¹‹å‰ï¼Œå®ƒè¿˜ä¼šè¶Šè¿‡é›¶ç‚¹å‡ æ¬¡ã€‚

å°çš„å­¦ä¹ ç‡ä¼šå¯¼è‡´æ”¶æ•›é€Ÿåº¦éå¸¸æ…¢ã€‚å¦‚æœè¿­ä»£æ¬¡æ•°æœ‰é™ï¼Œåˆ™ç®—æ³•å¯èƒ½ä¼šåœ¨æ‰¾åˆ°æœ€å°å€¼ä¹‹å‰è¿”å›ã€‚å¦åˆ™ï¼Œæ•´ä¸ªè¿‡ç¨‹å¯èƒ½ä¼šèŠ±è´¹ä¸å¯æ¥å—çš„å¤§é‡æ—¶é—´ã€‚ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œå†æ¬¡è¿è¡Œ`gradient_descent()`,è¿™ä¸€æ¬¡å­¦ä¹ ç‡å°å¾—å¤šï¼Œä¸º 0.005:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005
... )
6.050060671375367
```

ç°åœ¨çš„ç»“æœæ˜¯`6.05`ï¼Œå®ƒç¦»çœŸæ­£çš„æœ€å°å€¼é›¶å¾ˆè¿œã€‚è¿™æ˜¯å› ä¸ºç”±äºå­¦ä¹ ç‡å°ï¼Œå‘é‡çš„å˜åŒ–éå¸¸å°:

[![gda-small-learning-rate](img/b8c15faf28f66046d12a7cc6a972feea.png)](https://files.realpython.com/media/gd-4.9a5c436570fd.png)

åƒä»¥å‰ä¸€æ ·ï¼Œæœç´¢è¿‡ç¨‹ä»ğ‘£ = 10 å¼€å§‹ï¼Œä½†æ˜¯å®ƒä¸èƒ½åœ¨äº”åæ¬¡è¿­ä»£ä¸­åˆ°è¾¾é›¶ã€‚ç„¶è€Œï¼Œç»è¿‡ 100 æ¬¡è¿­ä»£ï¼Œè¯¯å·®ä¼šå°å¾—å¤šï¼Œç»è¿‡ 1000 æ¬¡è¿­ä»£ï¼Œè¯¯å·®ä¼šéå¸¸æ¥è¿‘äºé›¶:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,
...     n_iter=100
... )
3.660323412732294
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,
...     n_iter=1000
... )
0.0004317124741065828
>>> gradient_descent(
...     gradient=lambda v: 2 * v, start=10.0, learn_rate=0.005,
...     n_iter=2000
... )
9.952518849647663e-05
```

éå‡¸å‡½æ•°å¯èƒ½æœ‰å±€éƒ¨æå°å€¼æˆ–éç‚¹ï¼Œç®—æ³•å¯èƒ½ä¼šé™·å…¥å…¶ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯¹å­¦ä¹ é€Ÿç‡æˆ–èµ·ç‚¹çš„é€‰æ‹©å¯ä»¥å†³å®šæ‰¾åˆ°å±€éƒ¨æœ€å°å€¼è¿˜æ˜¯æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚

è€ƒè™‘å‡½æ•° [ğ‘£â´ - 5ğ‘£ - 3ğ‘£](https://www.wolframalpha.com/input/?i=v**4+-+5+*+v**2+-+3+*+v) ã€‚å®ƒåœ¨ğ‘£æœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼â‰ˆ 1.7ï¼Œåœ¨ğ‘£æœ‰ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼â‰ˆ1.42ã€‚è¿™ä¸ªå‡½æ•°çš„æ¢¯åº¦æ˜¯ 4ğ‘£10ğ‘£3ã€‚è®©æˆ‘ä»¬çœ‹çœ‹`gradient_descent()`åœ¨è¿™é‡Œæ˜¯å¦‚ä½•å·¥ä½œçš„:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,
...     learn_rate=0.2
... )
-1.4207567437458342
```

ä½ è¿™æ¬¡ä»é›¶å¼€å§‹ï¼Œç®—æ³•åœ¨å±€éƒ¨æœ€å°å€¼é™„è¿‘ç»“æŸã€‚ä¸‹é¢æ˜¯å¼•æ“ç›–ä¸‹å‘ç”Ÿçš„äº‹æƒ…:

[![gda-local-minimum](img/e795fc796ece964ff0b52bbcb2640a51.png)](https://files.realpython.com/media/gd-7.67e03e9337db.png)

åœ¨å‰ä¸¤æ¬¡è¿­ä»£ä¸­ï¼Œä½ çš„å‘é‡å‘å…¨å±€æœ€å°å€¼ç§»åŠ¨ï¼Œä½†ä¹‹åå®ƒè¶Šè¿‡å¦ä¸€è¾¹ï¼Œåœç•™åœ¨å±€éƒ¨æœ€å°å€¼ã€‚ä½ å¯ä»¥ç”¨è¾ƒå°çš„å­¦ä¹ ç‡æ¥é˜²æ­¢è¿™ç§æƒ…å†µ:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: 4 * v**3 - 10 * v - 3, start=0,
...     learn_rate=0.1
... )
1.285401330315467
```

å½“ä½ å°†å­¦ä¹ ç‡ä»`0.2`é™ä½åˆ°`0.1`æ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªéå¸¸æ¥è¿‘å…¨å±€æœ€å°å€¼çš„è§£ã€‚è®°ä½æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§è¿‘ä¼¼çš„æ–¹æ³•ã€‚è¿™ä¸€æ¬¡ï¼Œä½ é¿å…è·³åˆ°å¦ä¸€è¾¹:

[![gda-global-minimum](img/f93fe7d645ada689c4e06e3daf8f1e6b.png)](https://files.realpython.com/media/gd-8.f055cad0b634.png)

è¾ƒä½çš„å­¦ä¹ ç‡é˜²æ­¢å‘é‡è¿›è¡Œå¤§çš„è·³è·ƒï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‘é‡ä¿æŒæ›´æ¥è¿‘å…¨å±€æœ€ä¼˜ã€‚

è°ƒæ•´å­¦ä¹ é€Ÿåº¦æ˜¯å¾ˆæ£˜æ‰‹çš„ã€‚ä½ æ— æ³•é¢„å…ˆçŸ¥é“æœ€ä½³å€¼ã€‚æœ‰è®¸å¤šæŠ€æœ¯å’Œè¯•æ¢æ³•è¯•å›¾å¯¹æ­¤æœ‰æ‰€å¸®åŠ©ã€‚æ­¤å¤–ï¼Œæœºå™¨å­¦ä¹ å®è·µè€…ç»å¸¸åœ¨æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°æœŸé—´è°ƒæ•´å­¦ä¹ ç‡ã€‚

é™¤äº†å­¦ä¹ é€Ÿåº¦ä¹‹å¤–ï¼Œèµ·ç‚¹ä¹Ÿä¼šæ˜¾è‘—å½±å“è§£ï¼Œå°¤å…¶æ˜¯å¯¹äºéå‡¸å‡½æ•°ã€‚

[*Remove ads*](/account/join/)

## æ¢¯åº¦ä¸‹é™ç®—æ³•çš„åº”ç”¨

åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°ä¸¤ä¸ªä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„ç®€çŸ­ç¤ºä¾‹ã€‚æ‚¨è¿˜å°†äº†è§£åˆ°å®ƒå¯ä»¥ç”¨äºç°å®ç”Ÿæ´»ä¸­çš„æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œå¦‚çº¿æ€§å›å½’ã€‚åœ¨ç¬¬äºŒç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦ä¿®æ”¹`gradient_descent()`çš„ä»£ç ï¼Œå› ä¸ºæ‚¨éœ€è¦æ¥è‡ªè§‚å¯Ÿçš„æ•°æ®æ¥è®¡ç®—æ¢¯åº¦ã€‚

### ç®€çŸ­ç¤ºä¾‹

é¦–å…ˆï¼Œæ‚¨å°†æŠŠ`gradient_descent()`åº”ç”¨äºå¦ä¸€ä¸ªä¸€ç»´é—®é¢˜ã€‚å–å‡½æ•°[ğ‘£log(ğ‘£)](https://www.wolframalpha.com/input/?i=v+-+log%28v%29)ã€‚è¯¥å‡½æ•°çš„æ¢¯åº¦ä¸º 1 1/ğ‘£.æœ‰äº†è¿™äº›ä¿¡æ¯ï¼Œä½ å¯ä»¥æ‰¾åˆ°å®ƒçš„æœ€å°å€¼:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: 1 - 1 / v, start=2.5, learn_rate=0.5
... )
1.0000011077232125
```

åˆ©ç”¨æä¾›çš„ä¸€ç»„å‚æ•°ï¼Œ`gradient_descent()`æ­£ç¡®åœ°è®¡ç®—å‡ºè¯¥å‡½æ•°åœ¨ğ‘£ = 1 æ—¶å…·æœ‰æœ€å°å€¼ã€‚ä½ å¯ä»¥ç”¨å­¦ä¹ ç‡å’Œèµ·ç‚¹çš„å…¶ä»–å€¼æ¥è¯•è¯•ã€‚

æ‚¨ä¹Ÿå¯ä»¥å°†`gradient_descent()`ç”¨äºå¤šä¸ªå˜é‡çš„å‡½æ•°ã€‚åº”ç”¨ç¨‹åºæ˜¯ç›¸åŒçš„ï¼Œä½†æ˜¯æ‚¨éœ€è¦ä»¥å‘é‡æˆ–æ•°ç»„çš„å½¢å¼æä¾›æ¸å˜å’Œèµ·å§‹ç‚¹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°å…·æœ‰æ¢¯åº¦å‘é‡(2ğ‘£â‚ï¼Œ4ğ‘£â‚‚)çš„å‡½æ•° [ğ‘£â‚ + ğ‘£â‚‚â´](https://www.wolframalpha.com/input/?i=v_1**2+%2B+v_2**4) çš„æœ€å°å€¼:

>>>

```py
>>> gradient_descent(
...     gradient=lambda v: np.array([2 * v[0], 4 * v[1]**3]),
...     start=np.array([1.0, 1.0]), learn_rate=0.2, tolerance=1e-08
... )
array([8.08281277e-12, 9.75207120e-02])
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ çš„æ¢¯åº¦å‡½æ•°è¿”å›ä¸€ä¸ªæ•°ç»„ï¼Œå¼€å§‹å€¼æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæ‰€ä»¥ä½ å¾—åˆ°ä¸€ä¸ªæ•°ç»„ä½œä¸ºç»“æœã€‚å¾—åˆ°çš„å€¼å‡ ä¹ç­‰äºé›¶ï¼Œæ‰€ä»¥ä½ å¯ä»¥è¯´`gradient_descent()`æ­£ç¡®åœ°å‘ç°äº†è¿™ä¸ªå‡½æ•°çš„æœ€å°å€¼åœ¨ğ‘£â‚ = ğ‘£â‚‚ = 0ã€‚

### æ™®é€šæœ€å°äºŒä¹˜æ³•

æ­£å¦‚ä½ å·²ç»çŸ¥é“çš„ï¼Œçº¿æ€§å›å½’å’Œæ™®é€šæœ€å°äºŒä¹˜æ³•ä»è¾“å…¥ğ± = (ğ‘¥â‚ï¼Œâ€¦ï¼Œğ‘¥áµ£)å’Œè¾“å‡ºğ‘¦.çš„è§‚å¯Ÿå€¼å¼€å§‹ä»–ä»¬å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§å‡½æ•°ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£ï¼Œå°½å¯èƒ½æ¥è¿‘ğ‘¦.

è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚å®ƒä¼šæ‰¾åˆ°æœ€å°åŒ–æ®‹å·®å¹³æ–¹å’Œ SSR =Ïƒáµ¢(ğ‘¦áµ¢ğ‘“(ğ±áµ¢)æˆ–å‡æ–¹è¯¯å·® MSE = SSR / ğ‘›.çš„æƒé‡å€¼ğ‘â‚€ã€ğ‘â‚ã€â€¦ã€ğ‘áµ£è¿™é‡Œï¼Œğ‘›æ˜¯è§‚æµ‹å€¼çš„æ€»æ•°ï¼Œğ‘– = 1ï¼Œâ€¦ï¼Œğ‘›.

ä¹Ÿå¯ä»¥ä½¿ç”¨æˆæœ¬å‡½æ•°ğ¶ = SSR / (2ğ‘›)ï¼Œè¿™åœ¨æ•°å­¦ä¸Šæ¯” SSR æˆ– MSE æ›´æ–¹ä¾¿ã€‚

çº¿æ€§å›å½’æœ€åŸºæœ¬çš„å½¢å¼æ˜¯[ç®€å•çº¿æ€§å›å½’](https://realpython.com/linear-regression-in-python/#simple-linear-regression)ã€‚å®ƒåªæœ‰ä¸€ç»„è¾“å…¥ğ‘¥å’Œä¸¤ä¸ªæƒé‡:ğ‘â‚€å’Œğ‘â‚.å›å½’çº¿çš„æ–¹ç¨‹æ˜¯ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥.è™½ç„¶ğ‘â‚€å’Œğ‘â‚çš„æœ€ä½³å€¼å¯ä»¥é€šè¿‡åˆ†æè®¡ç®—å¾—åˆ°ï¼Œä½†æ˜¯æ‚¨å°†ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥ç¡®å®šå®ƒä»¬ã€‚

é¦–å…ˆï¼Œä½ éœ€è¦å¾®ç§¯åˆ†æ¥å¯»æ‰¾æˆæœ¬å‡½æ•°ğ¶=Ïƒáµ¢(ğ‘¦áµ¢ğ‘â‚€ğ‘â‚ğ‘¥áµ¢)/(2ğ‘›).)çš„æ¢¯åº¦å› ä¸ºä½ æœ‰ä¸¤ä¸ªå†³ç­–å˜é‡ï¼Œğ‘â‚€å’Œğ‘â‚ï¼Œæ¢¯åº¦âˆ‡ğ¶æ˜¯ä¸€ä¸ªå‘é‡æœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†:

1.  âˆ‚ğ¶/âˆ‚ğ‘â‚€ = (1/ğ‘›) Î£áµ¢(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) = mean(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢)
2.  âˆ‚ğ¶/âˆ‚ğ‘â‚ = (1/ğ‘›) Î£áµ¢(ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) ğ‘¥áµ¢ = mean((ğ‘â‚€ + ğ‘â‚ğ‘¥áµ¢ âˆ’ ğ‘¦áµ¢) ğ‘¥áµ¢)

ä½ éœ€è¦ğ‘¥å’Œğ‘¦çš„å€¼æ¥è®¡ç®—è¿™ä¸ªæˆæœ¬å‡½æ•°çš„æ¢¯åº¦ã€‚æ¢¯åº¦å‡½æ•°çš„è¾“å…¥ä¸ä»…åŒ…æ‹¬ğ‘â‚€å’Œğ‘â‚ï¼Œè¿˜åŒ…æ‹¬ğ‘¥å’Œğ‘¦.è¿™å¯èƒ½æ˜¯å®ƒçœ‹èµ·æ¥çš„æ ·å­:

```py
def ssr_gradient(x, y, b):
    res = b[0] + b[1] * x - y
    return res.mean(), (res * x).mean()  # .mean() is a method of np.ndarray
```

`ssr_gradient()`è·å–æ•°ç»„`x`å’Œ`y`ï¼Œå®ƒä»¬åŒ…å«è§‚å¯Ÿè¾“å…¥å’Œè¾“å‡ºï¼Œä»¥åŠä¿å­˜å†³ç­–å˜é‡ğ‘â‚€å’Œğ‘â‚.çš„å½“å‰å€¼çš„æ•°ç»„`b`è¯¥å‡½æ•°é¦–å…ˆè®¡ç®—æ¯ä¸ªè§‚æµ‹å€¼çš„æ®‹å·®æ•°ç»„(`res`)ï¼Œç„¶åè¿”å›âˆ‚ğ¶/âˆ‚ğ‘â‚€å’Œâˆ‚ğ¶/âˆ‚ğ‘â‚.çš„ä¸€å¯¹å€¼

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–¹ä¾¿çš„ NumPy æ–¹æ³• [`ndarray.mean()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.mean.html) ï¼Œå› ä¸ºæ‚¨å°† NumPy æ•°ç»„ä½œä¸ºå‚æ•°ä¼ é€’ã€‚

`gradient_descent()`éœ€è¦ä¸¤ä¸ªå°è°ƒæ•´:

1.  åœ¨ç¬¬ 4 è¡Œå¢åŠ `x`å’Œ`y`ä½œä¸º`gradient_descent()`çš„å‚æ•°ã€‚
2.  å°†`x`å’Œ`y`æä¾›ç»™æ¸å˜å‡½æ•°ï¼Œå¹¶ç¡®ä¿åœ¨ç¬¬ 8 è¡Œå°†æ¸å˜å…ƒç»„è½¬æ¢ä¸º NumPy æ•°ç»„ã€‚

ä»¥ä¸‹æ˜¯`gradient_descent()`å¯¹è¿™äº›å˜åŒ–çš„çœ‹æ³•:

```py
 1import numpy as np
 2
 3def gradient_descent(
 4    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06 5):
 6    vector = start
 7    for _ in range(n_iter):
 8        diff = -learn_rate * np.array(gradient(x, y, vector)) 9        if np.all(np.abs(diff) <= tolerance):
10            break
11        vector += diff
12    return vector
```

`gradient_descent()`ç°åœ¨æ¥å—è§‚å¯Ÿè¾“å…¥`x`å’Œè¾“å‡º`y`ï¼Œå¹¶å¯ä»¥ä½¿ç”¨å®ƒä»¬æ¥è®¡ç®—æ¢¯åº¦ã€‚å°†`gradient(x, y, vector)`çš„è¾“å‡ºè½¬æ¢æˆä¸€ä¸ª NumPy æ•°ç»„ï¼Œä½¿å¾—æ¢¯åº¦å…ƒç´ å¯ä»¥æŒ‰å…ƒç´ ä¹˜ä»¥å­¦ä¹ ç‡ï¼Œè¿™åœ¨å•å˜é‡å‡½æ•°çš„æƒ…å†µä¸‹æ˜¯ä¸å¿…è¦çš„ã€‚

ç°åœ¨åº”ç”¨ä½ çš„æ–°ç‰ˆæœ¬çš„`gradient_descent()`æ¥å¯»æ‰¾`x`å’Œ`y`çš„ä»»æ„å€¼çš„å›å½’çº¿:

>>>

```py
>>> x = np.array([5, 15, 25, 35, 45, 55])
>>> y = np.array([5, 20, 14, 32, 22, 38])

>>> gradient_descent(
...     ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,
...     n_iter=100_000
... )
array([5.62822349, 0.54012867])
```

ç»“æœæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå…¶ä¸­æœ‰ä¸¤ä¸ªå€¼å¯¹åº”äºå†³ç­–å˜é‡:ğ‘â‚€ = 5.63ï¼Œğ‘â‚ = 0.54ã€‚æœ€ä½³å›å½’çº¿æ˜¯ğ‘“(ğ‘¥) = 5.63 + 0.54ğ‘¥.å’Œå‰é¢çš„ä¾‹å­ä¸€æ ·ï¼Œè¿™ä¸ªç»“æœå¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå­¦ä¹ é€Ÿåº¦ã€‚å­¦ä¹ ç‡å¤ªä½æˆ–å¤ªé«˜ï¼Œä½ éƒ½å¯èƒ½å¾—ä¸åˆ°è¿™ä¹ˆå¥½çš„ç»“æœã€‚

è¿™ä¸ªä¾‹å­å¹¶ä¸å®Œå…¨æ˜¯éšæœºçš„â€”â€”å®ƒæ‘˜è‡ª Python ä¸­çš„æ•™ç¨‹[çº¿æ€§å›å½’ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œæ‚¨å·²ç»è·å¾—äº†ä¸æ¥è‡ª scikit-learn](https://realpython.com/linear-regression-in-python/#simple-linear-regression-with-scikit-learn) çš„[çº¿æ€§å›å½’å™¨å‡ ä¹ç›¸åŒçš„ç»“æœã€‚æ•°æ®å’Œå›å½’ç»“æœæ˜¾ç¤ºåœ¨](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)[ç®€å•çº¿æ€§å›å½’](https://realpython.com/linear-regression-in-python/#simple-linear-regression)éƒ¨åˆ†ã€‚

[*Remove ads*](/account/join/)

### ä»£ç çš„æ”¹è¿›

æ‚¨å¯ä»¥åœ¨ä¸ä¿®æ”¹å…¶æ ¸å¿ƒåŠŸèƒ½çš„æƒ…å†µä¸‹ä½¿`gradient_descent()`æ›´åŠ å¥å£®ã€å…¨é¢å’Œç¾è§‚:

```py
 1import numpy as np
 2
 3def gradient_descent(
 4    gradient, x, y, start, learn_rate=0.1, n_iter=50, tolerance=1e-06,
 5    dtype="float64"
 6):
 7    # Checking if the gradient is callable
 8    if not callable(gradient):
 9        raise TypeError("'gradient' must be callable")
10
11    # Setting up the data type for NumPy arrays
12    dtype_ = np.dtype(dtype)
13
14    # Converting x and y to NumPy arrays
15    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
16    if x.shape[0] != y.shape[0]:
17        raise ValueError("'x' and 'y' lengths do not match")
18
19    # Initializing the values of the variables
20    vector = np.array(start, dtype=dtype_)
21
22    # Setting up and checking the learning rate
23    learn_rate = np.array(learn_rate, dtype=dtype_)
24    if np.any(learn_rate <= 0):
25        raise ValueError("'learn_rate' must be greater than zero")
26
27    # Setting up and checking the maximal number of iterations
28    n_iter = int(n_iter)
29    if n_iter <= 0:
30        raise ValueError("'n_iter' must be greater than zero")
31
32    # Setting up and checking the tolerance
33    tolerance = np.array(tolerance, dtype=dtype_)
34    if np.any(tolerance <= 0):
35        raise ValueError("'tolerance' must be greater than zero")
36
37    # Performing the gradient descent loop
38    for _ in range(n_iter):
39        # Recalculating the difference
40        diff = -learn_rate * np.array(gradient(x, y, vector), dtype_)
41
42        # Checking if the absolute difference is small enough
43        if np.all(np.abs(diff) <= tolerance):
44            break
45
46        # Updating the values of the variables
47        vector += diff
48
49    return vector if vector.shape else vector.item()
```

`gradient_descent()`ç°åœ¨æ¥å—ä¸€ä¸ªé¢å¤–çš„`dtype`å‚æ•°ï¼Œè¯¥å‚æ•°å®šä¹‰äº†å‡½æ•°ä¸­ NumPy æ•°ç»„çš„æ•°æ®ç±»å‹ã€‚å…³äº NumPy ç±»å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[å…³äºæ•°æ®ç±»å‹çš„å®˜æ–¹æ–‡æ¡£](https://numpy.org/doc/stable/user/basics.types.html)ã€‚

åœ¨å¤§å¤šæ•°åº”ç”¨ç¨‹åºä¸­ï¼Œæ‚¨ä¸ä¼šæ³¨æ„åˆ° 32 ä½å’Œ 64 ä½æµ®ç‚¹æ•°ä¹‹é—´çš„å·®å¼‚ï¼Œä½†æ˜¯å½“æ‚¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œè¿™å¯èƒ½ä¼šæ˜¾è‘—å½±å“å†…å­˜ä½¿ç”¨ï¼Œç”šè‡³å¯èƒ½ä¼šå½±å“[å¤„ç†é€Ÿåº¦](https://stackoverflow.com/questions/15340781/python-numpy-data-types-performance)ã€‚ä¾‹å¦‚ï¼Œè™½ç„¶ NumPy é»˜è®¤ä½¿ç”¨ 64 ä½æµ®ç‚¹æ•°ï¼Œ [TensorFlow ç»å¸¸ä½¿ç”¨ 32 ä½åè¿›åˆ¶æ•°](https://www.tensorflow.org/guide/tensor)ã€‚

é™¤äº†è€ƒè™‘æ•°æ®ç±»å‹ï¼Œä¸Šé¢çš„ä»£ç è¿˜å¼•å…¥äº†ä¸€äº›ä¸ç±»å‹æ£€æŸ¥å’Œç¡®ä¿ NumPy åŠŸèƒ½çš„ä½¿ç”¨ç›¸å…³çš„ä¿®æ”¹:

*   **ç¬¬ 8 è¡Œå’Œç¬¬ 9 è¡Œ**æ£€æŸ¥`gradient`æ˜¯å¦æ˜¯ Python çš„å¯è°ƒç”¨å¯¹è±¡ï¼Œä»¥åŠæ˜¯å¦å¯ä»¥ä½œä¸ºå‡½æ•°ä½¿ç”¨ã€‚å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆè¯¥åŠŸèƒ½ä¼šæŠ›å‡ºä¸€ä¸ª [`TypeError`](https://docs.python.org/3/library/exceptions.html#TypeError) ã€‚

*   **ç¬¬ 12 è¡Œ**è®¾ç½®äº†ä¸€ä¸ª [`numpy.dtype`](https://numpy.org/doc/stable/reference/generated/numpy.dtype.html) çš„å®ä¾‹ï¼Œè¯¥å®ä¾‹å°†åœ¨æ•´ä¸ªå‡½æ•°ä¸­ç”¨ä½œæ‰€æœ‰æ•°ç»„çš„æ•°æ®ç±»å‹ã€‚

*   **ç¬¬ 15 è¡Œ**æ¥å—å‚æ•°`x`å’Œ`y`å¹¶äº§ç”Ÿå…·æœ‰æ‰€éœ€æ•°æ®ç±»å‹çš„ NumPy æ•°ç»„ã€‚å‚æ•°`x`å’Œ`y`å¯ä»¥æ˜¯åˆ—è¡¨ã€å…ƒç»„ã€æ•°ç»„æˆ–å…¶ä»–åºåˆ—ã€‚

*   **ç¬¬ 16 è¡Œå’Œç¬¬ 17 è¡Œ**æ¯”è¾ƒ`x`å’Œ`y`çš„å°ºå¯¸ã€‚è¿™å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæ‚¨å¸Œæœ›ç¡®ä¿ä¸¤ä¸ªæ•°ç»„å…·æœ‰ç›¸åŒæ•°é‡çš„è§‚å¯Ÿå€¼ã€‚å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆè¿™ä¸ªå‡½æ•°ä¼šæŠ›å‡ºä¸€ä¸ª [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError) ã€‚

*   **ç¬¬ 20 è¡Œ**å°†å‚æ•°`start`è½¬æ¢æˆä¸€ä¸ª NumPy æ•°ç»„ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„æŠ€å·§:å¦‚æœ`start`æ˜¯ä¸€ä¸ª Python æ ‡é‡ï¼Œé‚£ä¹ˆå®ƒå°†è¢«è½¬æ¢æˆä¸€ä¸ªç›¸åº”çš„ NumPy å¯¹è±¡(ä¸€ä¸ªåªæœ‰ä¸€ä¸ªå…ƒç´ å’Œé›¶ç»´çš„æ•°ç»„)ã€‚å¦‚æœä½ ä¼ é€’ä¸€ä¸ªåºåˆ—ï¼Œé‚£ä¹ˆå®ƒå°†å˜æˆä¸€ä¸ªå…·æœ‰ç›¸åŒæ•°é‡å…ƒç´ çš„å¸¸è§„ NumPy æ•°ç»„ã€‚

*   ç¬¬ 23 è¡Œå¯¹å­¦ä¹ ç‡åšäº†åŒæ ·çš„äº‹æƒ…ã€‚è¿™éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä½¿æ‚¨èƒ½å¤Ÿé€šè¿‡å‘`gradient_descent()`ä¼ é€’ä¸€ä¸ªåˆ—è¡¨ã€å…ƒç»„æˆ– NumPy æ•°ç»„æ¥ä¸ºæ¯ä¸ªå†³ç­–å˜é‡æŒ‡å®šä¸åŒçš„å­¦ä¹ ç‡ã€‚

*   **ç¬¬ 24 å’Œ 25 è¡Œ**æ£€æŸ¥å­¦ä¹ ç‡å€¼(æˆ–æ‰€æœ‰å˜é‡çš„å€¼)æ˜¯å¦å¤§äºé›¶ã€‚

*   **ç¬¬ 28 è¡Œåˆ°ç¬¬ 35 è¡Œ**åŒæ ·è®¾ç½®`n_iter`å’Œ`tolerance`å¹¶æ£€æŸ¥å®ƒä»¬æ˜¯å¦å¤§äºé›¶ã€‚

*   **ç¬¬ 38 è¡Œåˆ°ç¬¬ 47 è¡Œ**å’Œä¹‹å‰å·®ä¸å¤šã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯ç¬¬ 40 è¡Œçš„æ¸å˜æ•°ç»„çš„ç±»å‹ã€‚

*   å¦‚æœæœ‰å‡ ä¸ªå†³ç­–å˜é‡ï¼Œç¬¬ 49 è¡Œæ–¹ä¾¿åœ°è¿”å›ç»“æœæ•°ç»„ï¼›å¦‚æœåªæœ‰ä¸€ä¸ªå˜é‡ï¼Œåˆ™è¿”å› Python æ ‡é‡ã€‚

æ‚¨çš„`gradient_descent()`ç°åœ¨å·²ç»å®Œæˆã€‚éšæ„æ·»åŠ ä¸€äº›é¢å¤–çš„åŠŸèƒ½æˆ–æŠ›å…‰ã€‚æœ¬æ•™ç¨‹çš„ä¸‹ä¸€æ­¥æ˜¯ä½¿ç”¨ä½ åˆ°ç›®å‰ä¸ºæ­¢å­¦åˆ°çš„çŸ¥è¯†æ¥å®ç°æ¢¯åº¦ä¸‹é™çš„éšæœºç‰ˆæœ¬ã€‚

## éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•

**éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•**æ˜¯å¯¹æ¢¯åº¦ä¸‹é™çš„ä¸€ç§ä¿®æ”¹ã€‚åœ¨éšæœºæ¢¯åº¦ä¸‹é™ä¸­ï¼Œåªä½¿ç”¨éšæœºçš„ä¸€å°éƒ¨åˆ†è§‚æµ‹å€¼è€Œä¸æ˜¯å…¨éƒ¨è§‚æµ‹å€¼æ¥è®¡ç®—æ¢¯åº¦ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å‡å°‘è®¡ç®—æ—¶é—´ã€‚

**åœ¨çº¿éšæœºæ¢¯åº¦ä¸‹é™**æ˜¯éšæœºæ¢¯åº¦ä¸‹é™çš„ä¸€ç§å˜ä½“ï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæ‚¨å¯ä»¥ä¼°è®¡æ¯æ¬¡è§‚å¯Ÿçš„æˆæœ¬å‡½æ•°çš„æ¢¯åº¦ï¼Œå¹¶ç›¸åº”åœ°æ›´æ–°å†³ç­–å˜é‡ã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨æ‰¾åˆ°å…¨å±€æœ€å°å€¼ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡å‡½æ•°æ˜¯å‡¸çš„æƒ…å†µä¸‹ã€‚

**æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™**ä»‹äºæ™®é€šæ¢¯åº¦ä¸‹é™å’Œåœ¨çº¿æ–¹æ³•ä¹‹é—´ã€‚è®¡ç®—æ¢¯åº¦ï¼Œå¹¶ç”¨æ‰€æœ‰è§‚å¯Ÿå€¼çš„å­é›†è¿­ä»£æ›´æ–°å†³ç­–å˜é‡ï¼Œç§°ä¸º**å°æ‰¹**ã€‚è¿™ç§å˜ä½“åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ–¹é¢éå¸¸æµè¡Œã€‚

æ‚¨å¯ä»¥å°†åœ¨çº¿ç®—æ³•æƒ³è±¡æˆä¸€ç§ç‰¹æ®Šçš„æ‰¹å¤„ç†ç®—æ³•ï¼Œå…¶ä¸­æ¯ä¸ªå°æ‰¹åªæœ‰ä¸€ä¸ªè§‚å¯Ÿå€¼ã€‚ç»å…¸æ¢¯åº¦ä¸‹é™æ˜¯å¦ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå…¶ä¸­åªæœ‰ä¸€æ‰¹åŒ…å«æ‰€æœ‰è§‚æµ‹å€¼ã€‚

### éšæœºæ¢¯åº¦ä¸‹é™ä¸­çš„å°æ‰¹é‡

ä¸æ™®é€šæ¢¯åº¦ä¸‹é™çš„æƒ…å†µä¸€æ ·ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ä»å†³ç­–å˜é‡çš„åˆå§‹å‘é‡å¼€å§‹ï¼Œå¹¶é€šè¿‡å‡ æ¬¡è¿­ä»£æ¥æ›´æ–°å®ƒã€‚ä¸¤è€…çš„åŒºåˆ«åœ¨äºè¿­ä»£å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆ:

*   éšæœºæ¢¯åº¦ä¸‹é™éšæœºåœ°å°†è§‚å¯Ÿå€¼é›†åˆ†æˆå°æ‰¹ã€‚
*   å¯¹äºæ¯ä¸ªå°æ‰¹æ¬¡ï¼Œè®¡ç®—æ¢¯åº¦å¹¶ç§»åŠ¨å‘é‡ã€‚
*   ä¸€æ—¦æ‰€æœ‰çš„è¿·ä½ æ‰¹æ¬¡éƒ½è¢«ä½¿ç”¨ï¼Œä½ å°±è¯´è¿­ä»£ï¼Œæˆ–è€…è¯´**çºªå…ƒ**å·²ç»å®Œæˆï¼Œå¹¶å¼€å§‹ä¸‹ä¸€ä¸ªã€‚

è¯¥ç®—æ³•éšæœºé€‰æ‹©å°æ‰¹æ¬¡çš„è§‚å¯Ÿå€¼ï¼Œå› æ­¤æ‚¨éœ€è¦æ¨¡æ‹Ÿè¿™ç§éšæœº(æˆ–ä¼ªéšæœº)è¡Œä¸ºã€‚ä½ å¯ä»¥é€šè¿‡[éšæœºæ•°ç”Ÿæˆ](https://realpython.com/python-random/)æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚Python å†…ç½®äº† [`random`](https://docs.python.org/3/library/random.html) æ¨¡å—ï¼ŒNumPy è‡ªå¸¦[éšæœºç”Ÿæˆå™¨](https://numpy.org/doc/stable/reference/random/generator.html)ã€‚å½“æ‚¨ä½¿ç”¨æ•°ç»„æ—¶ï¼Œåè€…æ›´æ–¹ä¾¿ã€‚

æ‚¨å°†åˆ›å»ºä¸€ä¸ªåä¸º`sgd()`çš„æ–°å‡½æ•°ï¼Œå®ƒä¸`gradient_descent()`éå¸¸ç›¸ä¼¼ï¼Œä½†æ˜¯ä½¿ç”¨éšæœºé€‰æ‹©çš„å°å—åœ¨æœç´¢ç©ºé—´ä¸­ç§»åŠ¨:

```py
 1import numpy as np
 2
 3def sgd(
 4    gradient, x, y, start, learn_rate=0.1, batch_size=1, n_iter=50, 5    tolerance=1e-06, dtype="float64", random_state=None 6):
 7    # Checking if the gradient is callable
 8    if not callable(gradient):
 9        raise TypeError("'gradient' must be callable")
10
11    # Setting up the data type for NumPy arrays
12    dtype_ = np.dtype(dtype)
13
14    # Converting x and y to NumPy arrays
15    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
16    n_obs = x.shape[0] 17    if n_obs != y.shape[0]:
18        raise ValueError("'x' and 'y' lengths do not match")
19    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)] 20
21    # Initializing the random number generator
22    seed = None if random_state is None else int(random_state) 23    rng = np.random.default_rng(seed=seed) 24
25    # Initializing the values of the variables
26    vector = np.array(start, dtype=dtype_)
27
28    # Setting up and checking the learning rate
29    learn_rate = np.array(learn_rate, dtype=dtype_)
30    if np.any(learn_rate <= 0):
31        raise ValueError("'learn_rate' must be greater than zero")
32
33    # Setting up and checking the size of minibatches
34    batch_size = int(batch_size) 35    if not 0 < batch_size <= n_obs: 36        raise ValueError( 37            "'batch_size' must be greater than zero and less than " 38            "or equal to the number of observations" 39        ) 40
41    # Setting up and checking the maximal number of iterations
42    n_iter = int(n_iter)
43    if n_iter <= 0:
44        raise ValueError("'n_iter' must be greater than zero")
45
46    # Setting up and checking the tolerance
47    tolerance = np.array(tolerance, dtype=dtype_)
48    if np.any(tolerance <= 0):
49        raise ValueError("'tolerance' must be greater than zero")
50
51    # Performing the gradient descent loop
52    for _ in range(n_iter):
53        # Shuffle x and y
54        rng.shuffle(xy) 55
56        # Performing minibatch moves
57        for start in range(0, n_obs, batch_size): 58            stop = start + batch_size 59            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:] 60
61            # Recalculating the difference
62            grad = np.array(gradient(x_batch, y_batch, vector), dtype_) 63            diff = -learn_rate * grad 64
65            # Checking if the absolute difference is small enough
66            if np.all(np.abs(diff) <= tolerance):
67                break
68
69            # Updating the values of the variables
70            vector += diff
71
72    return vector if vector.shape else vector.item()
```

è¿™é‡Œæœ‰ä¸€ä¸ªæ–°å‚æ•°ã€‚ä½¿ç”¨`batch_size`ï¼Œæ‚¨å¯ä»¥æŒ‡å®šæ¯ä¸ªè¿·ä½ æ‰¹æ¬¡ä¸­çš„è§‚å¯Ÿæ¬¡æ•°ã€‚è¿™æ˜¯éšæœºæ¢¯åº¦ä¸‹é™çš„ä¸€ä¸ªé‡è¦å‚æ•°ï¼Œä¼šæ˜¾è‘—å½±å“æ€§èƒ½ã€‚ç¬¬ 34 åˆ° 39 è¡Œç¡®ä¿`batch_size`æ˜¯ä¸€ä¸ªä¸å¤§äºè§‚å¯Ÿæ€»æ•°çš„æ­£æ•´æ•°ã€‚

å¦ä¸€ä¸ªæ–°å‚æ•°æ˜¯`random_state`ã€‚å®ƒåœ¨ç¬¬ 22 è¡Œå®šä¹‰äº†éšæœºæ•°å‘ç”Ÿå™¨çš„ç§å­ã€‚ç¬¬ 23 è¡Œçš„ç§å­è¢«ç”¨ä½œ [`default_rng()`](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng) çš„å‚æ•°ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ª [`Generator`](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator) çš„å®ä¾‹ã€‚

å¦‚æœæ‚¨ä¸º`random_state`ä¼ é€’å‚æ•° [`None`](https://realpython.com/null-in-python/) ï¼Œé‚£ä¹ˆéšæœºæ•°ç”Ÿæˆå™¨å°†åœ¨æ¯æ¬¡å®ä¾‹åŒ–æ—¶è¿”å›ä¸åŒçš„æ•°å­—ã€‚å¦‚æœæ‚¨å¸Œæœ›ç”Ÿæˆå™¨çš„æ¯ä¸ªå®ä¾‹ä»¥å®Œå…¨ç›¸åŒçš„æ–¹å¼è¿è¡Œï¼Œé‚£ä¹ˆæ‚¨éœ€è¦æŒ‡å®š`seed`ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯æä¾›ä¸€ä¸ªä»»æ„æ•´æ•°ã€‚

ç¬¬ 16 è¡Œç”¨`x.shape[0]`å‡å»è§‚å¯Ÿæ¬¡æ•°ã€‚å¦‚æœ`x`æ˜¯ä¸€ç»´æ•°ç»„ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯å®ƒçš„å¤§å°ã€‚å¦‚æœ`x`æ˜¯äºŒç»´çš„ï¼Œé‚£ä¹ˆ`.shape[0]`å°±æ˜¯è¡Œæ•°ã€‚

åœ¨ç¬¬ 19 è¡Œï¼Œæ‚¨ä½¿ç”¨ [`.reshape()`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape) æ¥ç¡®ä¿`x`å’Œ`y`éƒ½æˆä¸ºå…·æœ‰`n_obs`è¡Œçš„äºŒç»´æ•°ç»„ï¼Œå¹¶ä¸”`y`æ­£å¥½æœ‰ä¸€åˆ—ã€‚ [`numpy.c_[]`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html#numpy.ndarray.reshape) æ–¹ä¾¿çš„å°†`x`å’Œ`y`çš„åˆ—ä¸²è”æˆä¸€ä¸ªæ•°ç»„`xy`ã€‚è¿™æ˜¯ä½¿æ•°æ®é€‚åˆéšæœºé€‰æ‹©çš„ä¸€ç§æ–¹æ³•ã€‚

æœ€åï¼Œåœ¨ç¬¬ 52 åˆ° 70 è¡Œï¼Œä½ å®ç°äº†éšæœºæ¢¯åº¦ä¸‹é™çš„ [`for`å¾ªç¯](https://realpython.com/python-for-loop/)ã€‚ä¸`gradient_descent()`ä¸åŒã€‚åœ¨ç¬¬ 54 è¡Œï¼Œæ‚¨ä½¿ç”¨éšæœºæ•°ç”Ÿæˆå™¨åŠå…¶æ–¹æ³• [`.shuffle()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.shuffle.html#numpy.random.Generator.shuffle) æ¥æ‰“ä¹±è§‚å¯Ÿç»“æœã€‚è¿™æ˜¯éšæœºé€‰æ‹©è¿·ä½ æ‰¹æ¬¡çš„æ–¹æ³•ä¹‹ä¸€ã€‚

æ¯ä¸ªè¿·ä½ æ‰¹æ¬¡éƒ½é‡å¤å†…éƒ¨`for`å¾ªç¯ã€‚ä¸æ™®é€šæ¢¯åº¦ä¸‹é™çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œåœ¨ç¬¬ 62 è¡Œï¼Œæ¢¯åº¦æ˜¯é’ˆå¯¹å°æ‰¹é‡çš„è§‚å¯Ÿå€¼(`x_batch`å’Œ`y_batch`)è®¡ç®—çš„ï¼Œè€Œä¸æ˜¯é’ˆå¯¹æ‰€æœ‰è§‚å¯Ÿå€¼(`x`å’Œ`y`)ã€‚

åœ¨ç¬¬ 59 è¡Œï¼Œ`x_batch`æˆä¸º`xy`çš„ä¸€éƒ¨åˆ†ï¼ŒåŒ…å«å½“å‰è¿·ä½ æ‰¹å¤„ç†çš„è¡Œ(ä»`start`åˆ°`stop`)å’Œå¯¹åº”äº`x`çš„åˆ—ã€‚`y_batch`ä¿å­˜ä¸`xy`ç›¸åŒçš„è¡Œï¼Œä½†åªä¿å­˜æœ€åä¸€åˆ—(è¾“å‡º)ã€‚æœ‰å…³ NumPy ä¸­ç´¢å¼•å¦‚ä½•å·¥ä½œçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§å…³äºç´¢å¼•çš„å®˜æ–¹æ–‡æ¡£ã€‚

ç°åœ¨ï¼Œæ‚¨å¯ä»¥æµ‹è¯•æ‚¨çš„éšæœºæ¢¯åº¦ä¸‹é™å®ç°äº†:

>>>

```py
>>> sgd(
...     ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,
...     batch_size=3, n_iter=100_000, random_state=0
... )
array([5.63093736, 0.53982921])
```

ç»“æœå’Œä½ ç”¨`gradient_descent()`å¾—åˆ°çš„å·®ä¸å¤šã€‚å¦‚æœä½ çœç•¥äº†`random_state`æˆ–è€…ä½¿ç”¨äº†`None`ï¼Œé‚£ä¹ˆæ¯æ¬¡è¿è¡Œ`sgd()`æ—¶ä½ ä¼šå¾—åˆ°ç¨å¾®ä¸åŒçš„ç»“æœï¼Œå› ä¸ºéšæœºæ•°å‘ç”Ÿå™¨ä¼šä»¥ä¸åŒçš„æ–¹å¼æ´—ç‰Œ`xy`ã€‚

[*Remove ads*](/account/join/)

### éšæœºæ¢¯åº¦ä¸‹é™ä¸­çš„åŠ¨é‡

æ­£å¦‚ä½ å·²ç»çœ‹åˆ°çš„ï¼Œå­¦ä¹ ç‡å¯¹æ¢¯åº¦ä¸‹é™çš„ç»“æœæœ‰å¾ˆå¤§çš„å½±å“ã€‚åœ¨ç®—æ³•æ‰§è¡ŒæœŸé—´ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‡ ç§ä¸åŒçš„ç­–ç•¥æ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚ä½ ä¹Ÿå¯ä»¥å°†**åŠ¨é‡**åº”ç”¨åˆ°ä½ çš„ç®—æ³•ä¸­ã€‚

ä½ å¯ä»¥ç”¨åŠ¨é‡æ¥ä¿®æ­£å­¦ä¹ ç‡çš„å½±å“ã€‚å…¶æ€æƒ³æ˜¯è®°ä½å‘é‡çš„å‰ä¸€æ¬¡æ›´æ–°ï¼Œå¹¶åœ¨è®¡ç®—ä¸‹ä¸€æ¬¡æ›´æ–°æ—¶åº”ç”¨å®ƒã€‚ä½ ä¸éœ€è¦ç²¾ç¡®åœ°åœ¨è´Ÿæ¢¯åº¦çš„æ–¹å‘ä¸Šç§»åŠ¨çŸ¢é‡ï¼Œä½†æ˜¯ä½ ä¹Ÿå€¾å‘äºä¿æŒå‰ä¸€æ¬¡ç§»åŠ¨çš„æ–¹å‘å’Œå¤§å°ã€‚

ç§°ä¸º**è¡°å‡ç‡**æˆ–**è¡°å‡å› å­**çš„å‚æ•°å®šä¹‰äº†å…ˆå‰æ›´æ–°çš„è´¡çŒ®æœ‰å¤šå¼ºã€‚è¦åŒ…æ‹¬åŠ¨é‡å’Œè¡°å‡ç‡ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ å‚æ•°`decay_rate`æ¥ä¿®æ”¹`sgd()`ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è®¡ç®—çŸ¢é‡æ›´æ–°çš„æ–¹å‘å’Œå¤§å°(`diff`):

```py
 1import numpy as np
 2
 3def sgd(
 4    gradient, x, y, start, learn_rate=0.1, decay_rate=0.0, batch_size=1, 5    n_iter=50, tolerance=1e-06, dtype="float64", random_state=None
 6):
 7    # Checking if the gradient is callable
 8    if not callable(gradient):
 9        raise TypeError("'gradient' must be callable")
10
11    # Setting up the data type for NumPy arrays
12    dtype_ = np.dtype(dtype)
13
14    # Converting x and y to NumPy arrays
15    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
16    n_obs = x.shape[0]
17    if n_obs != y.shape[0]:
18        raise ValueError("'x' and 'y' lengths do not match")
19    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]
20
21    # Initializing the random number generator
22    seed = None if random_state is None else int(random_state)
23    rng = np.random.default_rng(seed=seed)
24
25    # Initializing the values of the variables
26    vector = np.array(start, dtype=dtype_)
27
28    # Setting up and checking the learning rate
29    learn_rate = np.array(learn_rate, dtype=dtype_)
30    if np.any(learn_rate <= 0):
31        raise ValueError("'learn_rate' must be greater than zero")
32
33    # Setting up and checking the decay rate
34    decay_rate = np.array(decay_rate, dtype=dtype_) 35    if np.any(decay_rate < 0) or np.any(decay_rate > 1): 36        raise ValueError("'decay_rate' must be between zero and one") 37
38    # Setting up and checking the size of minibatches
39    batch_size = int(batch_size)
40    if not 0 < batch_size <= n_obs:
41        raise ValueError(
42            "'batch_size' must be greater than zero and less than "
43            "or equal to the number of observations"
44        )
45
46    # Setting up and checking the maximal number of iterations
47    n_iter = int(n_iter)
48    if n_iter <= 0:
49        raise ValueError("'n_iter' must be greater than zero")
50
51    # Setting up and checking the tolerance
52    tolerance = np.array(tolerance, dtype=dtype_)
53    if np.any(tolerance <= 0):
54        raise ValueError("'tolerance' must be greater than zero")
55
56    # Setting the difference to zero for the first iteration
57    diff = 0 58
59    # Performing the gradient descent loop
60    for _ in range(n_iter):
61        # Shuffle x and y
62        rng.shuffle(xy)
63
64        # Performing minibatch moves
65        for start in range(0, n_obs, batch_size):
66            stop = start + batch_size
67            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]
68
69            # Recalculating the difference
70            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)
71            diff = decay_rate * diff - learn_rate * grad 72
73            # Checking if the absolute difference is small enough
74            if np.all(np.abs(diff) <= tolerance):
75                break
76
77            # Updating the values of the variables
78            vector += diff
79
80    return vector if vector.shape else vector.item()
```

åœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œæ‚¨åœ¨ç¬¬ 4 è¡Œæ·»åŠ äº†`decay_rate`å‚æ•°ï¼Œåœ¨ç¬¬ 34 è¡Œå°†å…¶è½¬æ¢ä¸ºæ‰€éœ€ç±»å‹çš„ NumPy æ•°ç»„ï¼Œå¹¶åœ¨ç¬¬ 35 å’Œ 36 è¡Œæ£€æŸ¥å®ƒæ˜¯å¦ä»‹äº 0 å’Œ 1 ä¹‹é—´ã€‚åœ¨ç¬¬ 57 è¡Œï¼Œåœ¨è¿­ä»£å¼€å§‹ä¹‹å‰åˆå§‹åŒ–`diff`,ä»¥ç¡®ä¿å®ƒåœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­å¯ç”¨ã€‚

æœ€é‡è¦çš„å˜åŒ–å‘ç”Ÿåœ¨ç¬¬ 71 è¡Œã€‚ä½ ç”¨å­¦ä¹ ç‡å’Œæ¢¯åº¦é‡æ–°è®¡ç®—`diff`ï¼Œä½†ä¹ŸåŠ ä¸Šè¡°å‡ç‡å’Œæ—§å€¼`diff`çš„ä¹˜ç§¯ã€‚ç°åœ¨`diff`æœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†:

1.  **`decay_rate * diff`** æ˜¯æ°”åŠ¿ï¼Œæˆ–å†²å‡»å‰äººçš„ä¸¾åŠ¨ã€‚
2.  **`-learn_rate * grad`** æ˜¯å½“å‰æ¸å˜çš„å½±å“ã€‚

è¡°å‡ç‡å’Œå­¦ä¹ ç‡ç”¨ä½œå®šä¹‰ä¸¤è€…è´¡çŒ®çš„æƒé‡ã€‚

### éšæœºèµ·å§‹å€¼

ä¸æ™®é€šçš„æ¢¯åº¦ä¸‹é™ç›¸åï¼Œéšæœºæ¢¯åº¦ä¸‹é™çš„èµ·ç‚¹é€šå¸¸ä¸é‚£ä¹ˆé‡è¦ã€‚å¯¹äºç”¨æˆ·æ¥è¯´ï¼Œè¿™ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªä¸å¿…è¦çš„å›°éš¾ï¼Œå°¤å…¶æ˜¯å½“ä½ æœ‰å¾ˆå¤šå†³ç­–å˜é‡çš„æ—¶å€™ã€‚ä¸ºäº†å¾—åˆ°ä¸€ä¸ªæ¦‚å¿µï¼Œæƒ³è±¡ä¸€ä¸‹å¦‚æœä½ éœ€è¦æ‰‹åŠ¨åˆå§‹åŒ–ä¸€ä¸ªæœ‰æ•°åƒä¸ªåå·®å’Œæƒé‡çš„ç¥ç»ç½‘ç»œçš„å€¼ï¼

åœ¨å®è·µä¸­ï¼Œå¯ä»¥ä»ä¸€äº›å°çš„ä»»æ„å€¼å¼€å§‹ã€‚æ‚¨å°†ä½¿ç”¨éšæœºæ•°ç”Ÿæˆå™¨æ¥è·å–å®ƒä»¬:

```py
 1import numpy as np
 2
 3def sgd(
 4    gradient, x, y, n_vars=None, start=None, learn_rate=0.1, 5    decay_rate=0.0, batch_size=1, n_iter=50, tolerance=1e-06,
 6    dtype="float64", random_state=None
 7):
 8    # Checking if the gradient is callable
 9    if not callable(gradient):
10        raise TypeError("'gradient' must be callable")
11
12    # Setting up the data type for NumPy arrays
13    dtype_ = np.dtype(dtype)
14
15    # Converting x and y to NumPy arrays
16    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)
17    n_obs = x.shape[0]
18    if n_obs != y.shape[0]:
19        raise ValueError("'x' and 'y' lengths do not match")
20    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]
21
22    # Initializing the random number generator
23    seed = None if random_state is None else int(random_state)
24    rng = np.random.default_rng(seed=seed)
25
26    # Initializing the values of the variables
27    vector = ( 28        rng.normal(size=int(n_vars)).astype(dtype_) 29        if start is None else 30        np.array(start, dtype=dtype_) 31    ) 32
33    # Setting up and checking the learning rate
34    learn_rate = np.array(learn_rate, dtype=dtype_)
35    if np.any(learn_rate <= 0):
36        raise ValueError("'learn_rate' must be greater than zero")
37
38    # Setting up and checking the decay rate
39    decay_rate = np.array(decay_rate, dtype=dtype_)
40    if np.any(decay_rate < 0) or np.any(decay_rate > 1):
41        raise ValueError("'decay_rate' must be between zero and one")
42
43    # Setting up and checking the size of minibatches
44    batch_size = int(batch_size)
45    if not 0 < batch_size <= n_obs:
46        raise ValueError(
47            "'batch_size' must be greater than zero and less than "
48            "or equal to the number of observations"
49        )
50
51    # Setting up and checking the maximal number of iterations
52    n_iter = int(n_iter)
53    if n_iter <= 0:
54        raise ValueError("'n_iter' must be greater than zero")
55
56    # Setting up and checking the tolerance
57    tolerance = np.array(tolerance, dtype=dtype_)
58    if np.any(tolerance <= 0):
59        raise ValueError("'tolerance' must be greater than zero")
60
61    # Setting the difference to zero for the first iteration
62    diff = 0
63
64    # Performing the gradient descent loop
65    for _ in range(n_iter):
66        # Shuffle x and y
67        rng.shuffle(xy)
68
69        # Performing minibatch moves
70        for start in range(0, n_obs, batch_size):
71            stop = start + batch_size
72            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]
73
74            # Recalculating the difference
75            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)
76            diff = decay_rate * diff - learn_rate * grad
77
78            # Checking if the absolute difference is small enough
79            if np.all(np.abs(diff) <= tolerance):
80                break
81
82            # Updating the values of the variables
83            vector += diff
84
85    return vector if vector.shape else vector.item()
```

ç°åœ¨æœ‰äº†æ–°çš„å‚æ•°`n_vars`ï¼Œå®ƒå®šä¹‰äº†é—®é¢˜ä¸­å†³ç­–å˜é‡çš„æ•°é‡ã€‚å‚æ•°`start`æ˜¯å¯é€‰çš„ï¼Œé»˜è®¤å€¼ä¸º`None`ã€‚ç¬¬ 27 åˆ° 31 è¡Œåˆå§‹åŒ–å†³ç­–å˜é‡çš„åˆå§‹å€¼:

*   å¦‚æœæ‚¨æä¾›äº†ä¸€ä¸ª`start`å€¼è€Œä¸æ˜¯`None`ï¼Œé‚£ä¹ˆå®ƒå°†è¢«ç”¨ä½œèµ·å§‹å€¼ã€‚
*   å¦‚æœ`start`æ˜¯`None`ï¼Œé‚£ä¹ˆæ‚¨çš„éšæœºæ•°ç”Ÿæˆå™¨ä½¿ç”¨[æ ‡å‡†æ­£æ€åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution)å’Œ NumPy æ–¹æ³• [`.normal()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html) åˆ›å»ºèµ·å§‹å€¼ã€‚

ç°åœ¨ç»™`sgd()`ä¸€ä¸ªæœºä¼š:

>>>

```py
>>> sgd(
...     ssr_gradient, x, y, n_vars=2, learn_rate=0.0001,
...     decay_rate=0.8, batch_size=3, n_iter=100_000, random_state=0
... )
array([5.63014443, 0.53901017])
```

ä½ ä¼šå†æ¬¡å¾—åˆ°ç›¸ä¼¼çš„ç»“æœã€‚

ä½ å·²ç»å­¦ä¼šäº†å¦‚ä½•ç¼–å†™å®ç°æ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™çš„å‡½æ•°ã€‚ä¸Šé¢çš„ä»£ç å¯ä»¥å˜å¾—æ›´åŠ å¥å£®å’Œå®Œå–„ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨è‘—åçš„æœºå™¨å­¦ä¹ åº“ä¸­æ‰¾åˆ°è¿™äº›æ–¹æ³•çš„ä¸åŒå®ç°ã€‚

## Keras å’Œå¼ é‡æµä¸­çš„æ¢¯åº¦ä¸‹é™

éšæœºæ¢¯åº¦ä¸‹é™è¢«å¹¿æ³›ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œã€‚ç¥ç»ç½‘ç»œçš„åº“é€šå¸¸å…·æœ‰åŸºäºéšæœºæ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–ç®—æ³•çš„ä¸åŒå˜ä½“ï¼Œä¾‹å¦‚:

*   åœ£ç»ã€‹å’Œã€Šå¤å…°ç»ã€‹ä¼ ç»Ÿä¸­ï¼‰äºšå½“ï¼ˆäººç±»ç¬¬ä¸€äººçš„åå­—
*   é˜¿è¾¾æ ¼æ‹‰å¾·
*   é˜¿è¾¾å¾·å°”å¡”
*   RMSProp

è¿™äº›ä¼˜åŒ–åº“é€šå¸¸åœ¨ç¥ç»ç½‘ç»œè½¯ä»¶è®­ç»ƒæ—¶å†…éƒ¨è°ƒç”¨ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¹Ÿå¯ä»¥ç‹¬ç«‹ä½¿ç”¨å®ƒä»¬:

>>>

```py
>>> import tensorflow as tf

>>> # Create needed objects
>>> sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
>>> var = tf.Variable(2.5)
>>> cost = lambda: 2 + var ** 2

>>> # Perform optimization
>>> for _ in range(100):
...     sgd.minimize(cost, var_list=[var])

>>> # Extract results
>>> var.numpy()
-0.007128528
>>> cost().numpy()
2.0000508
```

åœ¨æœ¬ä¾‹ä¸­ï¼Œæ‚¨é¦–å…ˆå¯¼å…¥`tensorflow`ï¼Œç„¶ååˆ›å»ºä¼˜åŒ–æ‰€éœ€çš„å¯¹è±¡:

*   **`sgd`** æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨çš„ä¸€ä¸ªå®ä¾‹ï¼Œå­¦ä¹ é€Ÿç‡ä¸º`0.1`ï¼ŒåŠ¨é‡ä¸º`0.9`ã€‚
*   **`var`** æ˜¯å†³ç­–å˜é‡çš„å®ä¾‹ï¼Œåˆå§‹å€¼ä¸º`2.5`ã€‚
*   **`cost`** æ˜¯æˆæœ¬å‡½æ•°ï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯å¹³æ–¹å‡½æ•°ã€‚

ä»£ç çš„ä¸»è¦éƒ¨åˆ†æ˜¯ä¸€ä¸ª`for`å¾ªç¯ï¼Œå®ƒåå¤è°ƒç”¨`.minimize()`å¹¶ä¿®æ”¹`var`å’Œ`cost`ã€‚ä¸€æ—¦å¾ªç¯ç»“æŸï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`.numpy()`è·å¾—å†³ç­–å˜é‡å’Œæˆæœ¬å‡½æ•°çš„å€¼ã€‚

ä½ å¯ä»¥åœ¨ [Keras](https://keras.io/api/optimizers/) å’Œ [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/) æ–‡æ¡£ä¸­æ‰¾åˆ°å…³äºè¿™äº›ç®—æ³•çš„æ›´å¤šä¿¡æ¯ã€‚æ–‡ç« [æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•æ¦‚è¿°](https://ruder.io/optimizing-gradient-descent/)æä¾›äº†æ¢¯åº¦ä¸‹é™å˜é‡çš„ç»¼åˆè§£é‡Šåˆ—è¡¨ã€‚

[*Remove ads*](/account/join/)

## ç»“è®º

ä½ ç°åœ¨çŸ¥é“ä»€ä¹ˆæ˜¯**æ¢¯åº¦ä¸‹é™**å’Œ**éšæœºæ¢¯åº¦ä¸‹é™**ç®—æ³•ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å·¥ä½œã€‚å®ƒä»¬å¹¿æ³›ç”¨äºäººå·¥ç¥ç»ç½‘ç»œçš„åº”ç”¨ç¨‹åºä¸­ï¼Œå¹¶åœ¨ Keras å’Œ TensorFlow ç­‰æµè¡Œçš„åº“ä¸­å®ç°ã€‚

**åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å·²ç»å­¦ä¹ äº†:**

*   å¦‚ä½•**ä¸ºæ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™ç¼–å†™è‡ªå·±çš„å‡½æ•°**
*   å¦‚ä½•**åº”ç”¨ä½ çš„å‡½æ•°**è§£å†³ä¼˜åŒ–é—®é¢˜
*   æ¢¯åº¦ä¸‹é™çš„**å…³é”®ç‰¹å¾å’Œæ¦‚å¿µ**æ˜¯ä»€ä¹ˆï¼Œæ¯”å¦‚å­¦ä¹ é€Ÿç‡æˆ–åŠ¨é‡ï¼Œä»¥åŠå®ƒçš„å±€é™æ€§

æ‚¨å·²ç»ä½¿ç”¨æ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™æ‰¾åˆ°å‡ ä¸ªå‡½æ•°çš„æœ€å°å€¼ï¼Œå¹¶æ‹Ÿåˆçº¿æ€§å›å½’é—®é¢˜ä¸­çš„å›å½’çº¿ã€‚æ‚¨è¿˜çœ‹åˆ°äº†å¦‚ä½•åº”ç”¨ TensorFlow ä¸­ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œçš„ç±»`SGD`ã€‚

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–æ„è§ï¼Œè¯·å†™åœ¨ä¸‹é¢çš„è¯„è®ºåŒºã€‚*******